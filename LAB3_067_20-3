import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras import backend as K
import numpy as np
import matplotlib.pyplot as plt

# Load CIFAR-10 dataset
(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()

# Normalize the images to range [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Resize the images to 64x64 for better compatibility with VAE
x_train = tf.image.resize(x_train, (64, 64))
x_test = tf.image.resize(x_test, (64, 64))

# Encoder
inputs = layers.Input(shape=(64, 64, 3))
x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
x = layers.MaxPooling2D()(x)
x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D()(x)
x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D()(x)
x = layers.Flatten()(x)
z_mean = layers.Dense(2, name='z_mean')(x)
z_log_var = layers.Dense(2, name='z_log_var')(x)

# Sampling function
def sampling(args):
    z_mean, z_log_var = args
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.random.normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

z = layers.Lambda(sampling, output_shape=(2,))([z_mean, z_log_var])

# Decoder
latent_inputs = layers.Input(shape=(2,))
x = layers.Dense(8 * 8 * 128, activation='relu')(latent_inputs)
x = layers.Reshape((8, 8, 128))(x)
x = layers.Conv2DTranspose(128, 3, activation='relu', padding='same')(x)
x = layers.UpSampling2D()(x)
x = layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)
x = layers.UpSampling2D()(x)
x = layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)
x = layers.UpSampling2D()(x)
outputs = layers.Conv2DTranspose(3, 3, activation='sigmoid', padding='same')(x)

# VAE model
class VAE(models.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    
    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstructed = self.decoder(z)
        self.add_loss(self.vae_loss(inputs, reconstructed, z_mean, z_log_var))
        return reconstructed
    
    def vae_loss(self, inputs, outputs, z_mean, z_log_var):
        # Reconstruction loss
        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(inputs, outputs), axis=(1, 2)))
        
        # KL divergence loss
        kl_loss = -0.5 * tf.reduce_mean(
            tf.reduce_sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        )
        
        return recon_loss + kl_loss

# Build Encoder Model
encoder = models.Model(inputs, [z_mean, z_log_var, z])

# Build VAE model
vae = VAE(encoder, decoder)

# Compile the VAE model using Adam optimizer
vae.compile(optimizer='adam')

# Train the VAE model
vae.fit(x_train, x_train, epochs=20, batch_size=64, validation_data=(x_test, x_test))

# Visualize the latent space by plotting the 2D latent variables (z_mean)
z_mean, _, _ = encoder.predict(x_test)

plt.figure(figsize=(10, 10))
plt.scatter(z_mean[:, 0], z_mean[:, 1], c='blue', alpha=0.5)
plt.title("Latent Space of the VAE")
plt.xlabel("z_mean[0]")
plt.ylabel("z_mean[1]")
plt.show()

# Generate new images by sampling from the latent space
def generate_images(n_images=10):
    random_latents = np.random.normal(size=(n_images, 2))
    generated_images = decoder.predict(random_latents)
    
    fig, axes = plt.subplots(1, n_images, figsize=(20, 2))
    for i, ax in enumerate(axes):
        ax.imshow(generated_images[i])
        ax.axis('off')
    plt.show()

generate_images(5)
